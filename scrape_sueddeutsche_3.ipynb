{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: schleife über alle drei bzw. vier text analysis klassen\n",
    "# immer nur eine klasse at a time, weil sonst die GPU zu voll wird,\n",
    "# dann immer ein artikel at a time (10 sind so das maximum, das auf einmal auf die GPU passt)\n",
    "# vectorizer bracuth am meisten, summarizer eh\n",
    "# dann normal über alle artikel laufen und hochladen, frage: gleich jeden artikel einzeln hochladen oder immer in chuncks\n",
    "# da man immer neu in keykloak einloggen muss, würde ich sagen, immer so 100 verarbeiten (einzeln) und dann im 100er pack hochladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_handling.DataDownload import DataDownloader\n",
    "from database_handling.DataHandleAndOtherHelpers import DataHandler\n",
    "from database_handling.DataUpload import DataUploader\n",
    "from database_handling.KeycloakLogin import KeycloakLogin\n",
    "from scrapers.SueddeutscheScraper import SueddeutscheScraper\n",
    "from text_analysis.NEExtractor import NEExtractor\n",
    "from text_analysis.Summarizer import Summarizer\n",
    "from text_analysis.TopicExtractor import TopicExtractor\n",
    "from text_analysis.Vectorizers import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import json\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sueddeutsche_scraper = SueddeutscheScraper()\n",
    "#sueddeutsche_scraper.start_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sueddeutsche_scraper.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screenshot_path = \"screenshot.png\"\n",
    "#sueddeutsche_scraper.driver.get_screenshot_as_file(screenshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_runs_articles = sueddeutsche_scraper.scrape_sueddeutsche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to your JSON dump file\n",
    "json_file_path = 'sueddeutsche_articles.json'\n",
    "\n",
    "# Open the file and load its content\n",
    "with open(json_file_path, 'r') as file:\n",
    "    this_runs_articles = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token for the database\n",
    "keycloak_login = KeycloakLogin()\n",
    "token = keycloak_login.return_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sueddeutsche urls that are already in the database\n",
    "data_downloader = DataDownloader(token)\n",
    "sueddeutsche_articles_in_db = data_downloader.get_content(url=\"https://www.sueddeutsche.de/\") # TODO: Durch config ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_extractor = NEExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_extractor = TopicExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name deepset/gbert-base. Creating a new one with MEAN pooling.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "No sentence-transformers model found with name T-Systems-onsite/german-roberta-sentence-transformer-v2. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name xlm-roberta-large. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name google/bigbird-roberta-large. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name severinsimmler/xlm-roberta-longformer-large-16384. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "#summarizer = Summarizer()\n",
    "vectorizer = Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the data handler\n",
    "data_handler = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the articles that are already in the database, only update the last_verification_date\n",
    "articles_for_last_verifcation_date_update = data_handler.find_scraped_articles_already_in_db(this_runs_articles, sueddeutsche_articles_in_db)\n",
    "# safe the responses to the last verification date update\n",
    "responses_to_last_verifcation_date_update = data_handler.patch_last_online_verification_date(token, articles_for_last_verifcation_date_update)\n",
    "# get the articles that are not yet in the database\n",
    "sueddeutsche_articles_not_yet_in_db = data_handler.find_scraped_articles_not_already_in_db(this_runs_articles, sueddeutsche_articles_in_db)\n",
    "sueddeutsche_articles_not_yet_in_db_list_of_dicts = [article for article in this_runs_articles if article['url'] in sueddeutsche_articles_not_yet_in_db]\n",
    "\n",
    "# Define the number of articles to process and upload per iteration\n",
    "articles_per_iteration = 10\n",
    "\n",
    "# Calculate the number of iterations\n",
    "iterations = len(sueddeutsche_articles_not_yet_in_db_list_of_dicts) // articles_per_iteration + (len(sueddeutsche_articles_not_yet_in_db_list_of_dicts) % articles_per_iteration > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = sueddeutsche_articles_not_yet_in_db_list_of_dicts[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_and_free_memory(obj):\n",
    "    \"\"\"\n",
    "    Deletes the given object and frees GPU memory.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object to delete.\n",
    "    \"\"\"\n",
    "    del obj  # Delete the object\n",
    "    torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "    gc.collect()  # Force Python to clean up unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each article individually\n",
    "for i, article in enumerate(sueddeutsche_articles_not_yet_in_db_list_of_dicts):\n",
    "    print(f\"Processing article {i + 1} of {len(sueddeutsche_articles_not_yet_in_db_list_of_dicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 73 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
      "Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 21 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 116 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 41 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 37 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 26 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 20 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 13 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 30 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 98 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 10 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 25 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 93 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 32 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 53 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 17 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 47 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 92 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 46 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 108 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 69 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 61 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 59 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 48 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 42 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 45 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 35 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 39 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 36 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 34 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 31 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 23 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 19 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 15 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 9 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 4 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 18 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #articles = entity_extractor.extract_entities(articles)\n",
    "    #articles = topic_extractor.extract_topics(articles)\n",
    "    articles = vectorizer.vectorize(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = None\n",
    "del vectorizer  # Delete the list holding articles\n",
    "torch.cuda.empty_cache()  # Clears the GPU cache (if using PyTorch)\n",
    "gc.collect()  # Force Python to clean up unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_internal_vars = ['_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___']\n",
    "for var in list(locals().keys()):\n",
    "    if var not in jupyter_internal_vars and var not in ['torch', 'gc']:\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', '_i', '_ii', '_iii', '_i1', 'DataDownloader', 'DataHandler', 'DataUploader', 'KeycloakLogin', 'SueddeutscheScraper', 'NEExtractor', 'Summarizer', 'TopicExtractor', 'Vectorizer', '_i2', 'transformers', 'json', 'torch', 'gc', '_i3', 'json_file_path', 'file', 'this_runs_articles', '_i4', 'keycloak_login', 'token', '_i5', 'data_downloader', 'sueddeutsche_articles_in_db', '_i6', 'entity_extractor', 'topic_extractor', 'vectorizer', '_i7', 'data_handler', '_i8', 'articles_for_last_verifcation_date_update', 'responses_to_last_verifcation_date_update', 'sueddeutsche_articles_not_yet_in_db', 'sueddeutsche_articles_not_yet_in_db_list_of_dicts', 'articles_per_iteration', 'iterations', '_i9', 'articles', '_i10'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    print(\"Processing articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(sueddeutsche_articles_not_yet_in_db_list_of_dicts))\n",
    "    # Get the articles for this iteration\n",
    "    articles = sueddeutsche_articles_not_yet_in_db_list_of_dicts[i*articles_per_iteration:(i+1)*articles_per_iteration]\n",
    "\n",
    "    print(\"Running text processing on articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(sueddeutsche_articles_not_yet_in_db_list_of_dicts))\n",
    "    # Add the summaries, named entities, topics, and vectors to the articles dict\n",
    "    #articles = summarizer.summarize(articles)\n",
    "    with torch.no_grad():\n",
    "        articles = entity_extractor.extract_entities(articles)\n",
    "        articles = topic_extractor.extract_topics(articles)\n",
    "        articles = vectorizer.vectorize(articles)\n",
    "\n",
    "    # Remove main_text and lead_text from articles\n",
    "    for article in articles:\n",
    "        article.pop('main_text', None)\n",
    "        article.pop('lead_text', None)\n",
    "\n",
    "\n",
    "    print(\"Uploading articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(sueddeutsche_articles_not_yet_in_db_list_of_dicts))\n",
    "    # Ensure that the token is still valid every n iterations\n",
    "    # TODO: Tell Mario chuncking was done because I get a new token every 30 uploads to make sure the token is always valid\n",
    "    # if we do that every 1 upload that takes much longer since it takes ~20 seconds to get a net token/ensure the token is valid\n",
    "    keycloak_login = KeycloakLogin()\n",
    "    token = keycloak_login.return_token()\n",
    "    \n",
    "    # Loop over articles and put every article into the database\n",
    "    data_uploader = DataUploader(token)\n",
    "\n",
    "    responses = []\n",
    "    for article in articles:\n",
    "        response = data_uploader.post_content(article)\n",
    "        responses.append(response)\n",
    "\n",
    "    print(\"Processed and uploaded articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(sueddeutsche_articles_not_yet_in_db_list_of_dicts))\n",
    "\n",
    "    # Free up VRAM\n",
    "    print_vram_usage()\n",
    "    del articles  # Delete the list holding articles\n",
    "    torch.cuda.empty_cache()  # Clears the GPU cache (if using PyTorch)\n",
    "    gc.collect()  # Force Python to clean up unused memory\n",
    "    print_vram_usage()\n",
    "    \n",
    "    # save the responses to a json file\n",
    "    with open('responses.json', 'w') as f:\n",
    "        json.dump(responses, f)\n",
    "\n",
    "    # Free up VRAM\n",
    "    print_vram_usage()\n",
    "    del responses  # Delete the list holding articles\n",
    "    torch.cuda.empty_cache()  # Clears the GPU cache (if using PyTorch)\n",
    "    gc.collect()  # Force Python to clean up unused memory\n",
    "    print_vram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    model(input)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vram_usage()\n",
    "del articles  # Delete the list holding articles\n",
    "torch.cuda.empty_cache()  # Clears the GPU cache (if using PyTorch)\n",
    "gc.collect()  # Force Python to clean up unused memory\n",
    "print_vram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write articles to json file\n",
    "#import json\n",
    "#with open('sueddeutsche_articles.json', 'w') as f:\n",
    "#    json.dump(articles, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
