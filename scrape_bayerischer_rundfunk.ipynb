{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba7eecf-dfea-4544-8596-179dfd687634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ra76lax\\AppData\\Local\\anaconda3\\envs\\python_m3_scrapers\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ra76lax\\AppData\\Local\\anaconda3\\envs\\python_m3_scrapers\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ra76lax\\AppData\\Local\\anaconda3\\envs\\python_m3_scrapers\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "INFO:datasets:PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "from database_handling.DataDownload import DataDownloader\n",
    "from database_handling.DataHandleAndOtherHelpers import DataHandler\n",
    "from database_handling.DataUpload import DataUploader\n",
    "from database_handling.DataDelete import DataDeleter\n",
    "from database_handling.KeycloakLogin import KeycloakLogin\n",
    "from scrapers.BayerischerRundfunkScraper import BayerischerRundfunkScraper\n",
    "from text_analysis.NEExtractor import NEExtractor\n",
    "from text_analysis.Summarizer import Summarizer\n",
    "from text_analysis.TopicExtractor import TopicExtractor\n",
    "from text_analysis.Vectorizers import Vectorizer\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import transformers\n",
    "import json\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259fcba7-b545-40b1-bc5f-60c5c464a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff439fec-27ac-4368-8309-7549544166c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3506eb92-becc-42d9-a066-4e536f4dae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = BayerischerRundfunkScraper(headless=False)\n",
    "scraper.start_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8563683e-d55d-4092-b02c-ec013bf25367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapers.BaseScraper:Navigated to https://www.br.de/nachrichten/\n"
     ]
    }
   ],
   "source": [
    "# in the other scrapers the navigate_to function is called in the login() function\n",
    "# since bayerischer rundfunk does not have a paywall and login, we just navigate to the start page\n",
    "scraper.navigate_to(\"https://www.br.de/nachrichten/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6461ed15-9125-459f-8f93-af11b5ae171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.click_cookie_button()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af918d35-2f37-4499-a914-02ffe8d49e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the time of finalizing the scraper for bayerischer rundfunk the website introduced dark mode (only a few days ago). users can choose between light, system, and dark\n",
    "# I'm very sure the popup for choosing will not show up like this indefineteley, but at some point they will default to system\n",
    "# therefore the follwing line is used right here rather then being implemented in the actual scraper class somewhere\n",
    "scraper.driver.find_element(By.CSS_SELECTOR, \".RedesignModal_saveButton__nIBZN\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ade9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "subpage_urls = scraper._get_subpage_urls_on_current_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fc51d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subpage_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_found_urls = scraper.get_article_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cade106-b392-4364-8b48-24328a839f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "this_runs_articles = scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c9eeab-d60b-4da4-9f62-86ff614ce251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and load its content\n",
    "with open(\"bayerischer_rundfunk_articles.json\", 'r') as file:\n",
    "    this_runs_articles = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73294ee6-45c1-4b0e-a872-e790f22714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_runs_articles = this_runs_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e9d23a-319d-446f-ae23-486238e8e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a json file\n",
    "#with open('bayerischer_rundfunk_articles.json', 'w') as f:\n",
    "#    json.dump(this_runs_articles, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b2926d-b10c-441a-a60c-009897376483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token for the database\n",
    "keycloak_login = KeycloakLogin()\n",
    "token = keycloak_login.return_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bf3eef-45fb-4a6b-810b-096fed7a7341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from database_handling.DataDownload import DataDownloader\n",
    "\n",
    "test_url_checker = DataDownloader(token)\n",
    "status_code = test_url_checker.get_content_rehydrate_status_code_only(url=\"https://www.spiegel.de/deinspiegel/wie-der-wolf-zum-hund-wurde-der-beginn-einer-wunderbaren-freundschaft-a-4388fdf5-aeef-40f0-9d9c-753f3de38f9c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810e4154-8e3b-40e7-b26e-16208c883886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035f5613-56fd-4f63-baa3-273b7ed8b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e28d2d8-fddb-4540-83e6-bca1a79c2c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "head() got multiple values for argument 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.m3.ifkw.lmu.de/api/v1/content/rehydrate/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Send the HEAD request with the optional query parameters\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.spiegel.de/deinspiegel/wie-der-wolf-zum-hund-wurde-der-beginn-einer-wunderbaren-freundschaft-a-4388fdf5-aeef-40f0-9d9c-753f3de38f9c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: head() got multiple values for argument 'url'"
     ]
    }
   ],
   "source": [
    "# Construct the full URL for the API request\n",
    "api_url = f\"https://api.m3.ifkw.lmu.de/api/v1/content/rehydrate/\"\n",
    "\n",
    "# Send the HEAD request with the optional query parameters\n",
    "response = requests.head(api_url, url=\"https://www.spiegel.de/deinspiegel/wie-der-wolf-zum-hund-wurde-der-beginn-einer-wunderbaren-freundschaft-a-4388fdf5-aeef-40f0-9d9c-753f3de38f9c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add8ddc0-aa2f-4978-9229-118f2ae0c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles fetched: 737\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data downloader with the provided token\n",
    "data_downloader = DataDownloader(token)\n",
    "\n",
    "# Initialize variables for pagination\n",
    "all_articles_in_db = []\n",
    "page_size = 100  # Assuming the server returns 100 items per page by default\n",
    "offset = 0\n",
    "url = \"https://www.br.de/nachrichten/\"\n",
    "\n",
    "# Loop until all pages are retrieved\n",
    "while True:\n",
    "    # Fetch articles from the current page\n",
    "    articles_in_db = data_downloader.get_content(url=url, limit=page_size, offset=offset)\n",
    "    \n",
    "    # Check if articles were returned\n",
    "    if not articles_in_db or 'items' not in articles_in_db:\n",
    "        break\n",
    "    \n",
    "    # Add the current batch of articles to the complete list\n",
    "    all_articles_in_db.extend(articles_in_db['items'])\n",
    "    \n",
    "    # Update the offset for the next page\n",
    "    offset += page_size\n",
    "    \n",
    "    # Check if we have fetched all articles\n",
    "    if len(all_articles_in_db) >= articles_in_db['count']:\n",
    "        break\n",
    "\n",
    "# The total number of articles retrieved\n",
    "article_count = len(all_articles_in_db)\n",
    "\n",
    "print(f\"Total articles fetched: {article_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a50eccb-a834-49f8-9d1a-6014abaa1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the data handler\n",
    "data_handler = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75744cf4-af27-4b1c-bbd6-8f00a71b1915",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for the articles that are already in the database, only update the last_verification_date\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m articles_for_last_verifcation_date_update \u001b[38;5;241m=\u001b[39m \u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_scraped_articles_already_in_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_runs_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_articles_in_db\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# safe the responses to the last verification date update\u001b[39;00m\n\u001b[1;32m      4\u001b[0m responses_to_last_verifcation_date_update \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mpatch_last_online_verification_date(token, articles_for_last_verifcation_date_update)\n",
      "File \u001b[0;32m~/work/Patrick/m3-scraper/database_handling/DataHandleAndOtherHelpers.py:22\u001b[0m, in \u001b[0;36mDataHandler.find_scraped_articles_already_in_db\u001b[0;34m(self, scraped_articles, articles_already_in_db)\u001b[0m\n\u001b[1;32m     19\u001b[0m db_format_urls_list \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m articles_already_in_db]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Scraper format\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m scraper_format_urls_list \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m scraped_articles]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Find matching URLs, those scraped URLs that are already in the database\u001b[39;00m\n\u001b[1;32m     25\u001b[0m scraped_urls_already_in_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(db_format_urls_list)\u001b[38;5;241m.\u001b[39mintersection(scraper_format_urls_list)\n",
      "File \u001b[0;32m~/work/Patrick/m3-scraper/database_handling/DataHandleAndOtherHelpers.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m db_format_urls_list \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m articles_already_in_db]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Scraper format\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m scraper_format_urls_list \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m scraped_articles]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Find matching URLs, those scraped URLs that are already in the database\u001b[39;00m\n\u001b[1;32m     25\u001b[0m scraped_urls_already_in_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(db_format_urls_list)\u001b[38;5;241m.\u001b[39mintersection(scraper_format_urls_list)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# for the articles that are already in the database, only update the last_verification_date\n",
    "articles_for_last_verifcation_date_update = data_handler.find_scraped_articles_already_in_db(this_runs_articles, all_articles_in_db)\n",
    "# safe the responses to the last verification date update\n",
    "responses_to_last_verifcation_date_update = data_handler.patch_last_online_verification_date(token, articles_for_last_verifcation_date_update)\n",
    "# get the articles that are not yet in the database\n",
    "articles_not_yet_in_db = data_handler.find_scraped_articles_not_already_in_db(this_runs_articles, all_articles_in_db)\n",
    "articles_not_yet_in_db_list_of_dicts = [article for article in this_runs_articles if article['url'] in articles_not_yet_in_db]\n",
    "\n",
    "# Define the number of articles to process and upload per iteration\n",
    "articles_per_iteration = 10\n",
    "\n",
    "# Calculate the number of iterations\n",
    "iterations = len(articles_not_yet_in_db_list_of_dicts) // articles_per_iteration + (len(articles_not_yet_in_db_list_of_dicts) % articles_per_iteration > 0)\n",
    "\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb05c50-3543-4902-abe0-c91faa389420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting entity extraction...\n",
      "2024-08-29 15:47:37,134 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, B-PER, E-PER, S-LOC, B-MISC, I-MISC, E-MISC, S-PER, B-ORG, E-ORG, S-ORG, I-ORG, B-LOC, E-LOC, S-MISC, I-PER, I-LOC, <START>, <STOP>\n",
      "Tagger instantiated successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38dd2ad8ecb4def8c18d511c933d749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity extraction completed.\n",
      "Starting topic extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c66f6988364df7a29f515885f5b23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name deepset/gbert-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic extraction completed.\n",
      "Starting vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name T-Systems-onsite/german-roberta-sentence-transformer-v2. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name xlm-roberta-large. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name google/bigbird-roberta-large. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name severinsimmler/xlm-roberta-longformer-large-16384. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9ad4e7620f43ceb2f63fa056ab07e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed.\n",
      "Beginning article upload...\n",
      "Article upload completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clears GPU memory and forces garbage collection.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def process_articles_in_batches(text_analysis_class, method_name, articles, batch_size):\n",
    "    \"\"\"Process articles in batches using the specified text analysis class and method.\"\"\"\n",
    "    processor = text_analysis_class()\n",
    "    method = getattr(processor, method_name)\n",
    "    \n",
    "    for i in range(0, len(articles), batch_size):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        batch = method(batch)\n",
    "        \n",
    "        # Reassign the processed batch back to the main list\n",
    "        articles[i:i + batch_size] = batch\n",
    "\n",
    "        del batch\n",
    "        clear_gpu_memory()  # Clear memory after each batch\n",
    "    \n",
    "    del processor  # Delete the processor instance to free up GPU memory\n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Define your batch size\n",
    "batch_size = len(articles_not_yet_in_db_list_of_dicts)  # Adjust this based on your GPU capacity\n",
    "\n",
    "# Process with Entity Extractor\n",
    "print(\"Starting entity extraction...\")\n",
    "process_articles_in_batches(NEExtractor, 'extract_entities', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Entity extraction completed.\")\n",
    "\n",
    "# Process with Topic Extractor\n",
    "print(\"Starting topic extraction...\")\n",
    "process_articles_in_batches(TopicExtractor, 'extract_topics', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Topic extraction completed.\")\n",
    "\n",
    "# Process with Vectorizer\n",
    "print(\"Starting vectorization...\")\n",
    "process_articles_in_batches(Vectorizer, 'vectorize', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Vectorization completed.\")\n",
    "\n",
    "# Remove main_text and lead_text from articles to save space before uploading\n",
    "for article in articles_not_yet_in_db_list_of_dicts:\n",
    "    article.pop('main_text', None)\n",
    "    article.pop('lead_text', None)\n",
    "\n",
    "# Upload each article to the database\n",
    "print(\"Beginning article upload...\")\n",
    "responses = []\n",
    "keycloak_login = KeycloakLogin()\n",
    "token = keycloak_login.return_token()\n",
    "data_uploader = DataUploader(token)\n",
    "\n",
    "for article in articles_not_yet_in_db_list_of_dicts:\n",
    "    response = data_uploader.post_content(article)\n",
    "    responses.append(response)\n",
    "\n",
    "# Save the responses to a JSON file\n",
    "with open('responses.json', 'w') as f:\n",
    "    json.dump(responses, f)\n",
    "\n",
    "print(\"Article upload completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd1498-12b3-4ca6-8352-67a119bfc8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c47e2-ca46-44dd-8457-90bc4de266b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f3e1e-6c4f-4bd6-868d-875b22965348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(iterations):\n",
    "#    print(\"Processing articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "#    # Get the articles for this iteration\n",
    "#    articles = spiegel_articles_not_yet_in_db_list_of_dicts[i*articles_per_iteration:(i+1)*articles_per_iteration]#\n",
    "#\n",
    "#    print(\"Running text processing on articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    # Add the summaries, named entities, topics, and vectors to the articles dict\n",
    "    #articles = summarizer.summarize(articles)\n",
    "#    articles = entity_extractor.extract_entities(articles)\n",
    "#    articles = topic_extractor.extract_topics(articles)\n",
    "#    articles = vectorizer.vectorize(articles)\n",
    "\n",
    "    # Remove main_text and lead_text from articles\n",
    "#    for article in articles:\n",
    "#        article.pop('main_text', None)\n",
    "#        article.pop('lead_text', None)\n",
    "        \n",
    "#    print(\"Uploading articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    # Ensure that the token is still valid every n iterations\n",
    "    # TODO: Tell Mario chuncking was done because I get a new token every 30 uploads to make sure the token is always valid\n",
    "    # if we do that every 1 upload that takes much longer since it takes ~20 seconds to get a net token/ensure the token is valid#\n",
    "#    keycloak_login = KeycloakLogin()\n",
    "#    token = keycloak_login.return_token()\n",
    "    \n",
    "    # Loop over articles and put every article into the database\n",
    "#    data_uploader = DataUploader(token)\n",
    "    \n",
    "#    for article in articles:\n",
    "#        response = data_uploader.post_content(article)\n",
    "#        responses.append(response)\n",
    "    \n",
    "#    print(\"Processed and uploaded articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    \n",
    "        \n",
    "    # save the responses to a json file\n",
    "#    with open('responses.json', 'w') as f:\n",
    "#        json.dump(responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
