{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba7eecf-dfea-4544-8596-179dfd687634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from database_handling.DataDownload import DataDownloader\n",
    "from database_handling.DataHandleAndOtherHelpers import DataHandler\n",
    "from database_handling.DataUpload import DataUploader\n",
    "from database_handling.DataDelete import DataDeleter\n",
    "from database_handling.KeycloakLogin import KeycloakLogin\n",
    "from scrapers.SueddeutscheScraper import SueddeutscheScraper \n",
    "from text_analysis.NEExtractor import NEExtractor\n",
    "from text_analysis.Summarizer import Summarizer\n",
    "from text_analysis.TopicExtractor import TopicExtractor\n",
    "from text_analysis.Vectorizers import Vectorizer\n",
    "\n",
    "import transformers\n",
    "import json\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf8bcb2-8784-4df6-91cb-d42ded91c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = SueddeutscheScraper()\n",
    "scraper.start_browser()\n",
    "scraper.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0129daf2-e689-489a-be36-cfdadd84619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.driver.save_screenshot('screenshot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cade106-b392-4364-8b48-24328a839f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_runs_articles = scraper.scrape()\n",
    "# TODO: (maybe) check and validate articles that are in the database right here, maybe even before accessing it, only asking the outlet server if there is something there. \n",
    "# That might 1. decrease computational/scraping time a little and 2. decrease our footprint on the target server, if we don't scrape/fully access ~1000 articles every day but only the really news one\n",
    "# which are usually a lot let (50-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c9eeab-d60b-4da4-9f62-86ff614ce251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and load its content\n",
    "#with open(\"spiegel_articles.json\", 'r') as file:\n",
    "#    this_runs_articles = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e9d23a-319d-446f-ae23-486238e8e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the responses to a json file\n",
    "with open('sz_articles.json', 'w') as f:\n",
    "    json.dump(this_runs_articles, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20b19b2-6a98-4e39-a8bd-0c677a7f766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(this_runs_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bf3eef-45fb-4a6b-810b-096fed7a7341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_runs_articles[919]['main_text'].count('Tag 888')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fd6fe5-750b-4ade-9fa7-48b6e5cfe5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.sueddeutsche.de/muenchen/landkreismuenchen/ottobrunn-polizei-randalierer-radfahrer-trunkenheit-lux.Xbx87HSgD9fq3tU15JPxmB',\n",
       " 'main_text': 'Ein Polizist ist in der Nacht zum Mittwoch bei der Kontrolle eines 28-jährigen angetrunkenen, randalierenden Radfahrers in Ottobrunn schwerer verletzt worden. Drei Kollegen zogen sich leichte Verletzungen zu. Die Beamten hatten den Mann nach Angaben des Münchner Polizeipräsidiums zunächst gegen 22.45 Uhr angehalten und ihm ein verbotenes Einhandmesser abgenommen. Außerdem wiesen sie ihn darauf hin, dass er nicht mehr in der Lage sei, mit seinem Fahrrad zu fahren. Als er ihnen gegen 0 Uhr im Bereich der Kreuzung Mozartstraße und Altendorfstraße radelnd begegnete, eskalierte die Situation. Laut Polizeibericht wurde eine weitere Streife hinzugerufen und der 28-Jährige konnte erst nach minutenlanger Gegenwehr gefesselt werden. Er wurde unter anderem wegen Trunkenheit im Verkehr, Widerstands gegen Vollstreckungsbeamte und Beleidigung angezeigt. Die Münchner Kriminalpolizei ermittelt.',\n",
       " 'lead_text': 'Ein 28-Jähriger kann laut Polizei erst nach minutenlangem Ringen gefesselt werden. Ein Beamter zieht sich schwerere Blessuren zu.',\n",
       " 'last_online_verification_date': '2024-08-29T14:55:24.804122',\n",
       " 'medium': {'readable_id': 'sueddeutsche'},\n",
       " 'crawler_medium': 'sueddeutsche',\n",
       " 'crawler_version': '0.1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_runs_articles[918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b2926d-b10c-441a-a60c-009897376483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token for the database\n",
    "keycloak_login = KeycloakLogin()\n",
    "token = keycloak_login.return_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add8ddc0-aa2f-4978-9229-118f2ae0c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles fetched: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data downloader with the provided token\n",
    "data_downloader = DataDownloader(token)\n",
    "\n",
    "# Initialize variables for pagination\n",
    "all_articles_in_db = []\n",
    "page_size = 100  # Assuming the server returns 100 items per page by default\n",
    "offset = 0\n",
    "url = \"https://www.sueddeutsche.de/\"\n",
    "\n",
    "# Loop until all pages are retrieved\n",
    "while True:\n",
    "    # Fetch articles from the current page\n",
    "    articles_in_db = data_downloader.get_content(url=url, limit=page_size, offset=offset)\n",
    "    \n",
    "    # Check if articles were returned\n",
    "    if not articles_in_db or 'items' not in articles_in_db:\n",
    "        break\n",
    "    \n",
    "    # Add the current batch of articles to the complete list\n",
    "    all_articles_in_db.extend(articles_in_db['items'])\n",
    "    \n",
    "    # Update the offset for the next page\n",
    "    offset += page_size\n",
    "    \n",
    "    # Check if we have fetched all articles\n",
    "    if len(all_articles_in_db) >= articles_in_db['count']:\n",
    "        break\n",
    "\n",
    "# The total number of articles retrieved\n",
    "article_count = len(all_articles_in_db)\n",
    "\n",
    "print(f\"Total articles fetched: {article_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a50eccb-a834-49f8-9d1a-6014abaa1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the data handler\n",
    "data_handler = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75744cf4-af27-4b1c-bbd6-8f00a71b1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the articles that are already in the database, only update the last_verification_date\n",
    "articles_for_last_verifcation_date_update = data_handler.find_scraped_articles_already_in_db(this_runs_articles, all_articles_in_db)\n",
    "# safe the responses to the last verification date update\n",
    "responses_to_last_verifcation_date_update = data_handler.patch_last_online_verification_date(token, articles_for_last_verifcation_date_update)\n",
    "# get the articles that are not yet in the database\n",
    "articles_not_yet_in_db = data_handler.find_scraped_articles_not_already_in_db(this_runs_articles, all_articles_in_db)\n",
    "articles_not_yet_in_db_list_of_dicts = [article for article in this_runs_articles if article['url'] in articles_not_yet_in_db]\n",
    "\n",
    "# Define the number of articles to process and upload per iteration\n",
    "articles_per_iteration = 10\n",
    "\n",
    "# Calculate the number of iterations\n",
    "iterations = len(articles_not_yet_in_db_list_of_dicts) // articles_per_iteration + (len(articles_not_yet_in_db_list_of_dicts) % articles_per_iteration > 0)\n",
    "\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "244b46f5-3fb3-45a2-83eb-4a38451fc8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f5e4c4-a63b-4872-927a-6bf2c6e7e0ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m processor\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "del processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb05c50-3543-4902-abe0-c91faa389420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting entity extraction...\n",
      "2024-08-29 17:03:35,013 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, B-PER, E-PER, S-LOC, B-MISC, I-MISC, E-MISC, S-PER, B-ORG, E-ORG, S-ORG, I-ORG, B-LOC, E-LOC, S-MISC, I-PER, I-LOC, <START>, <STOP>\n",
      "Tagger instantiated successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811655d665f94645b8c28e9585d580dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1097 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f70ba33df30>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clears GPU memory and forces garbage collection.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def process_articles_in_batches(text_analysis_class, method_name, articles, batch_size):\n",
    "    \"\"\"Process articles in batches using the specified text analysis class and method.\"\"\"\n",
    "    processor = text_analysis_class()\n",
    "    method = getattr(processor, method_name)\n",
    "    \n",
    "    for i in range(0, len(articles), batch_size):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        batch = method(batch)\n",
    "        \n",
    "        # Reassign the processed batch back to the main list\n",
    "        articles[i:i + batch_size] = batch\n",
    "\n",
    "        del batch\n",
    "        clear_gpu_memory()  # Clear memory after each batch\n",
    "    \n",
    "    del processor  # Delete the processor instance to free up GPU memory\n",
    "    clear_gpu_memory()\n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Define your batch size\n",
    "batch_size = len(articles_not_yet_in_db_list_of_dicts)  # Adjust this based on your GPU capacity\n",
    "\n",
    "# Process with Entity Extractor\n",
    "print(\"Starting entity extraction...\")\n",
    "process_articles_in_batches(NEExtractor, 'extract_entities', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Entity extraction completed.\")\n",
    "\n",
    "# Process with Topic Extractor\n",
    "print(\"Starting topic extraction...\")\n",
    "process_articles_in_batches(TopicExtractor, 'extract_topics', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Topic extraction completed.\")\n",
    "\n",
    "# Process with Vectorizer\n",
    "print(\"Starting vectorization...\")\n",
    "process_articles_in_batches(Vectorizer, 'vectorize', articles_not_yet_in_db_list_of_dicts, batch_size)\n",
    "print(\"Vectorization completed.\")\n",
    "\n",
    "# Remove main_text and lead_text from articles to save space before uploading\n",
    "for article in articles_not_yet_in_db_list_of_dicts:\n",
    "    article.pop('main_text', None)\n",
    "    article.pop('lead_text', None)\n",
    "\n",
    "# Upload each article to the database\n",
    "print(\"Beginning article upload...\")\n",
    "responses = []\n",
    "keycloak_login = KeycloakLogin()\n",
    "token = keycloak_login.return_token()\n",
    "data_uploader = DataUploader(token)\n",
    "\n",
    "for article in articles_not_yet_in_db_list_of_dicts:\n",
    "    response = data_uploader.post_content(article)\n",
    "    responses.append(response)\n",
    "\n",
    "# Save the responses to a JSON file\n",
    "with open('responses.json', 'w') as f:\n",
    "    json.dump(responses, f)\n",
    "\n",
    "print(\"Article upload completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd1498-12b3-4ca6-8352-67a119bfc8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c47e2-ca46-44dd-8457-90bc4de266b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f3e1e-6c4f-4bd6-868d-875b22965348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(iterations):\n",
    "#    print(\"Processing articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "#    # Get the articles for this iteration\n",
    "#    articles = spiegel_articles_not_yet_in_db_list_of_dicts[i*articles_per_iteration:(i+1)*articles_per_iteration]#\n",
    "#\n",
    "#    print(\"Running text processing on articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    # Add the summaries, named entities, topics, and vectors to the articles dict\n",
    "    #articles = summarizer.summarize(articles)\n",
    "#    articles = entity_extractor.extract_entities(articles)\n",
    "#    articles = topic_extractor.extract_topics(articles)\n",
    "#    articles = vectorizer.vectorize(articles)\n",
    "\n",
    "    # Remove main_text and lead_text from articles\n",
    "#    for article in articles:\n",
    "#        article.pop('main_text', None)\n",
    "#        article.pop('lead_text', None)\n",
    "        \n",
    "#    print(\"Uploading articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    # Ensure that the token is still valid every n iterations\n",
    "    # TODO: Tell Mario chuncking was done because I get a new token every 30 uploads to make sure the token is always valid\n",
    "    # if we do that every 1 upload that takes much longer since it takes ~20 seconds to get a net token/ensure the token is valid#\n",
    "#    keycloak_login = KeycloakLogin()\n",
    "#    token = keycloak_login.return_token()\n",
    "    \n",
    "    # Loop over articles and put every article into the database\n",
    "#    data_uploader = DataUploader(token)\n",
    "    \n",
    "#    for article in articles:\n",
    "#        response = data_uploader.post_content(article)\n",
    "#        responses.append(response)\n",
    "    \n",
    "#    print(\"Processed and uploaded articles\", i*articles_per_iteration, \"to\", (i+1)*articles_per_iteration, \"from\", len(articles_not_yet_in_db_list_of_dicts))\n",
    "    \n",
    "        \n",
    "    # save the responses to a json file\n",
    "#    with open('responses.json', 'w') as f:\n",
    "#        json.dump(responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (m3_scraper)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
